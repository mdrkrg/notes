---
tags:
  - DataScience
  - SJTU-Statistics
---
5.1 大数定理预备知识
===
大数定理描述的是: 随着试验进行次数的提升, 频率会收敛到一个定值

## 关于期望的重要不等式
设非负连续性随机变量 $X$ 的期望 $E(X)$ 存在, 则对于任意实数 $\varepsilon > 0$,

$$P(X \geq \varepsilon) \leq \frac{E(X)}{\varepsilon}$$
```desmos-graph
top=1.2;bottom=-0.2
left=0
---
f(x) = \int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}*\exp(-\frac{(t-3)^2}{2})dt
g(x) = 1 - 1/(x-3)
```
## Markov 不等式
马尔可夫不等式描述的是非负随机变量绝对位置的概率上限

设随机变量 $X$ 的 $k$ 阶绝对原点矩 $E(|\,X\,|^k)$ 存在, 则对于任意实数 $\varepsilon >0$,

$$P(|X| \geq \varepsilon) \leq \frac{E\left(|X|^{k}\right)}{\varepsilon^{k}}$$

## Chebyshev 不等式
切比雪夫不等式描述的是随机变量距期望相对位置偏离的概率上限
- 距离期望越远, 随机变量**越不可能**取到这一点的值

设随机变量 $X$ 的期望 $E(X)=\mu$ , 方差 $D(X) = \sigma^2$, 则对于任意实数 $\varepsilon > 0$, 恒有
$$P(|X-\mu| \geqslant \varepsilon) \leqslant \frac{\sigma^{2}}{\varepsilon^{2}}$$

或
$$P(|X-\mu|<\varepsilon)>1-\frac{\sigma^{2}}{\varepsilon^{2}}$$
- 估计 $P(a< X < b)$ 的概率最好满足 $b - \mu = \mu - a$

## 依概率收敛
设 $Y_{1},Y_{2},\cdots,Y_{n},\cdots$ 是一个随机变量序列, $X$ 是一个随机变量, 

若 $\forall \varepsilon > 0$, 有
$$\lim _{n \rightarrow+\infty} P\left(\left|Y_{n}-X\right| \geqslant \varepsilon\right)=0$$

或
$$\lim _{n \rightarrow+\infty} P\left(\left|Y_{n}-X\right|<\varepsilon\right)=1$$

则称随机变量序列 $Y_{1},Y_{2},\cdots,Y_{n},\cdots$ 依概率收敛于 $X$, 记作 $Y_{n} \underset{n \rightarrow+\infty}{\stackrel{P}{\longrightarrow}} X$
